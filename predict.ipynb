{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "nuclear-diary",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import re\n",
    "import pandas as pd\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from hunspell import Hunspell\n",
    "import string\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import HashingVectorizer, CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# ---uncomment this code if the custom libraries needs to be reloaded from disk\n",
    "#import importlib\n",
    "#import src\n",
    "#importlib.reload(src.s3)\n",
    "#importlib.reload(src.transformation)\n",
    "#importlib.reload(src.tunning)\n",
    "\n",
    "# Custom libraries\n",
    "from src.transformation import TxtFeatureSelector, CategoricalFeatureSelector\n",
    "from src.s3 import get_file, get_comments\n",
    "\n",
    "h = Hunspell('es-CR', hunspell_data_dir='/home/ec2-user/SageMaker/user-feedback/include/huspell/dic/es-CR')\n",
    "spanishStemmer=SnowballStemmer(\"spanish\", ignore_stopwords=True)\n",
    "correctedToken = pd.read_json('/home/ec2-user/SageMaker/user-feedback/include/correction.json', orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "metropolitan-trail",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>creator_department</th>\n",
       "      <th>resource_type</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Customer Success</td>\n",
       "      <td>contact</td>\n",
       "      <td>Conectar la aplicación con Google Calendar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sales</td>\n",
       "      <td>contact</td>\n",
       "      <td>Cambiar el tamaño de la letra a la factura</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  creator_department resource_type                                     comment\n",
       "1   Customer Success       contact  Conectar la aplicación con Google Calendar\n",
       "2              Sales       contact  Cambiar el tamaño de la letra a la factura"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predict = pd.DataFrame(data=[\n",
    "    {\n",
    "        'creator_department': \"Customer Success\",\n",
    "        'resource_type': \"contact\",\n",
    "        'comment': \"Conectar la aplicación con Google Calendar\"\n",
    "    },\n",
    "    {\n",
    "        'creator_department': \"Sales\",\n",
    "        'resource_type': \"contact\",\n",
    "        'comment': \"Cambiar el tamaño de la letra a la factura\"\n",
    "    }\n",
    "], index=[1, 2])\n",
    "test_predict.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "distant-twelve",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>resource_type</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>74421406</th>\n",
       "      <td>contact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75323747</th>\n",
       "      <td>contact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75594152</th>\n",
       "      <td>contact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75594918</th>\n",
       "      <td>contact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75597209</th>\n",
       "      <td>deal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224560920</th>\n",
       "      <td>contact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224628362</th>\n",
       "      <td>lead</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225013006</th>\n",
       "      <td>deal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225018816</th>\n",
       "      <td>deal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225024817</th>\n",
       "      <td>lead</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1490 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       resource_type\n",
       "id                                  \n",
       "74421406   contact                  \n",
       "75323747   contact                  \n",
       "75594152   contact                  \n",
       "75594918   contact                  \n",
       "75597209   deal                     \n",
       "...                              ...\n",
       "224560920  contact                  \n",
       "224628362  lead                     \n",
       "225013006  deal                     \n",
       "225018816  deal                     \n",
       "225024817  lead                     \n",
       "\n",
       "[1490 rows x 1 columns]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variables['resource_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "meaningful-correction",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"/home/ec2-user/SageMaker/user-feedback/include/proccesed_comments.csv\", index_col='id')\n",
    "variables = data.drop('primary_category', axis = 1)\n",
    "response = data['primary_category'].values\n",
    "variables.loc[:,['resource_type']] = variables['resource_type'].apply(lambda x: x.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "adopted-caribbean",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing( BaseEstimator, TransformerMixin ):\n",
    "    # not used\n",
    "    def fit( self, X, y = None):\n",
    "        return self\n",
    "    \n",
    "    # return tokens column as received\n",
    "    def convert(self, X, CallBack):\n",
    "        return pd.DataFrame(X.apply(lambda row: CallBack(self.getToken(row)), axis=1), columns=['tokens'])\n",
    "\n",
    "    def getToken(self, row):\n",
    "        return row['tokens']\n",
    "\n",
    "class LowerCaser(Preprocessing):\n",
    "    # returns numpy.ndarray\n",
    "    def toLower(self, tokens):\n",
    "        return np.char.lower(tokens)\n",
    "    \n",
    "    # Convert the column to lower case\n",
    "    def transform( self, X, y = None ):\n",
    "        return self.convert(X=X, CallBack=self.toLower)\n",
    "    \n",
    "class PuntuationRemover(Preprocessing):\n",
    "    # returns numpy.ndarray\n",
    "    def removePuntuation(self, tokens):\n",
    "        result = np.empty((0,0), dtype=str, order='C')\n",
    "        for token in tokens:\n",
    "            if token not in string.punctuation:\n",
    "                result = np.append (result, token)\n",
    "        return result\n",
    "    \n",
    "    # remove puntuation symbols from the tokens\n",
    "    def transform( self, X, y = None ):\n",
    "        return self.convert(X=X, CallBack=self.removePuntuation)\n",
    "\n",
    "class Striper(Preprocessing):\n",
    "    # returns numpy.ndarray\n",
    "    def strip(self, tokens):\n",
    "        result = np.empty((0,0), dtype=str, order='C')\n",
    "        for token in tokens:\n",
    "            token = token.strip()\n",
    "            if token != \"\":\n",
    "                result = np.append (result, token)\n",
    "        return result\n",
    "    \n",
    "    # strip each token removing empty spaces\n",
    "    def transform( self, X, y = None ):\n",
    "        return self.convert(X=X, CallBack=self.strip)\n",
    "\n",
    "class StopWordsRemover(Preprocessing):\n",
    "    # returns numpy.ndarray\n",
    "    def removeStopWords(self, tokens):\n",
    "        result = np.empty((0,0), dtype=str, order='C')\n",
    "        for token in tokens:\n",
    "            if token in stopwords.words('spanish'):\n",
    "                continue\n",
    "            # remove the subjet of the sentences\n",
    "            if str(token).lower() in ['leonardo.quintanilla','hulihealth.com','dulce.rodriguez','liz.barrantes','hulilabs.com','doctor', 'doctora', 'dr', 'dra', 'secretaria', 'secretario', 'doctores', 'doctoras', 'cemim', 'http', 'https']:\n",
    "                continue\n",
    "            result = np.append (result, token)\n",
    "        return result\n",
    "    \n",
    "    # remove stops words from the sequence of tokens\n",
    "    def transform( self, X, y = None ):\n",
    "        return self.convert(X=X, CallBack=self.removeStopWords)\n",
    "\n",
    "class TokenCleaner(Preprocessing):\n",
    "    # returns numpy.ndarray\n",
    "    def tokenCleaner(self, tokens):\n",
    "        result = np.empty((0,0), dtype=str, order='C')\n",
    "        for token in tokens:\n",
    "            token = re.sub('[^A-Za-z]+', '', token)\n",
    "            if token.isalpha():\n",
    "                result = np.append (result, token)\n",
    "        return result\n",
    "    \n",
    "    # remove special characters from word\n",
    "    def transform(self, X, y = None ):\n",
    "        return self.convert(X=X, CallBack=self.tokenCleaner)\n",
    "    \n",
    "class WordCorrector(Preprocessing):\n",
    "    # returns numpy.ndarray\n",
    "    def correctWord(self, tokens):\n",
    "        result = np.empty((0,0), dtype=str, order='C')                \n",
    "        for token in tokens:\n",
    "            token = str(token)\n",
    "            if not h.spell(token):\n",
    "                if token in correctedToken.index:\n",
    "                    token = correctedToken.loc[token,:][0]\n",
    "                else:\n",
    "                    correction = h.suggest(token)\n",
    "                    # take the first suggestion as the corrected value\n",
    "                    if len(correction) != 0:\n",
    "                        token = correction[0]\n",
    "            # some of the token are identify to different words so we are separating those words\n",
    "            separatedTokens = word_tokenize(token)\n",
    "            result = np.append(result, separatedTokens)\n",
    "        return result\n",
    "    \n",
    "    # correct words in the sentence\n",
    "    def transform(self, X, y = None ):\n",
    "        return self.convert(X=X, CallBack=self.correctWord)\n",
    "    \n",
    "class Stemmer(Preprocessing):\n",
    "    # returns numpy.ndarray\n",
    "    def stemm(self, tokens):\n",
    "        result = np.empty((0,0), dtype=str, order='C')                \n",
    "        for token in tokens:\n",
    "            result = np.append(result, spanishStemmer.stem(str(token)))\n",
    "        return result\n",
    "    \n",
    "    # Get the root part of the word to reduce dimensionality\n",
    "    def transform(self, X, y = None ):\n",
    "        return self.convert(X=X, CallBack=self.stemm) \n",
    "\n",
    "class SingleLetterRemover(Preprocessing):\n",
    "    # returns numpy.ndarray\n",
    "    def removeSingleLetter(self, tokens):\n",
    "        result = np.empty((0,0), dtype=str, order='C')\n",
    "        for token in tokens:\n",
    "            if len(str(token)) > 1:\n",
    "                result = np.append(result, token)\n",
    "        return result\n",
    "    \n",
    "    # remove all single letter token\n",
    "    def transform(self, X, y = None ):\n",
    "        return self.convert(X=X, CallBack=self.removeSingleLetter)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "expanded-terror",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = ColumnTransformer([\n",
    "    ('preprocess', Pipeline(steps=[\n",
    "        ('LowerCaser', LowerCaser()),\n",
    "        ('PuntuationRemover', PuntuationRemover()),\n",
    "        ('Striper', Striper()),\n",
    "        ('TokenCleaner', TokenCleaner()),\n",
    "        ('WordCorrector', WordCorrector()),\n",
    "        ('StopWordsRemover', StopWordsRemover()),\n",
    "        ('SingleLetterRemover', SingleLetterRemover()),\n",
    "        ('Stemmer', Stemmer()),\n",
    "    ]), ['tokens']),\n",
    "], remainder='passthrough')\n",
    "test_predict['tokens'] = test_predict.apply(lambda row:  word_tokenize(row['comment']), axis=1)\n",
    "proccessed = preprocess.fit_transform(test_predict.reset_index())\n",
    "proccessed = pd.DataFrame(data=proccessed, columns=[\"tokens\",\"index\",\"creator_department\",\"resource_type\",\"comment\"])\n",
    "proccessed['comment'] = proccessed['tokens'].apply(lambda x: ' '.join(x))\n",
    "proccessed = proccessed[['creator_department','resource_type','comment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "large-record",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>creator_department</th>\n",
       "      <th>resource_type</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Customer Success</td>\n",
       "      <td>contact</td>\n",
       "      <td>conect aplic googl calendari</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sales</td>\n",
       "      <td>contact</td>\n",
       "      <td>cambi tam letr factur</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  creator_department resource_type                       comment\n",
       "0   Customer Success       contact  conect aplic googl calendari\n",
       "1              Sales       contact         cambi tam letr factur"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proccessed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innocent-bradford",
   "metadata": {},
   "source": [
    "# Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "waiting-custom",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_optimal_pipeline = Pipeline(steps = [\n",
    "    ('all',  FeatureUnion(transformer_list = [\n",
    "        ('cat_feature', Pipeline(steps = [\n",
    "            ('selector', CategoricalFeatureSelector()),\n",
    "            ('encoding', OneHotEncoder())\n",
    "        ])), \n",
    "       ('txt_feature', Pipeline(steps = [\n",
    "            ('selector', TxtFeatureSelector()),\n",
    "            ('vectorizer', TfidfVectorizer(ngram_range=(1,1), binary=False)),\n",
    "        ]))\n",
    "    ])),\n",
    "    #('fect_selec', SelectKBest(chi2, k=1200)),\n",
    "    ('model', SVC(kernel='linear', gamma=\"scale\", C=2, probability=True))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "educational-negative",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('all',\n",
       "                 FeatureUnion(n_jobs=None,\n",
       "                              transformer_list=[('cat_feature',\n",
       "                                                 Pipeline(memory=None,\n",
       "                                                          steps=[('selector',\n",
       "                                                                  CategoricalFeatureSelector()),\n",
       "                                                                 ('encoding',\n",
       "                                                                  OneHotEncoder(categories='auto',\n",
       "                                                                                drop=None,\n",
       "                                                                                dtype=<class 'numpy.float64'>,\n",
       "                                                                                handle_unknown='error',\n",
       "                                                                                sparse=True))],\n",
       "                                                          verbose=False)),\n",
       "                                                ('txt_feature',\n",
       "                                                 Pipeline(memory=None,\n",
       "                                                          steps=[('selec...\n",
       "                                                                                  tokenizer=None,\n",
       "                                                                                  use_idf=True,\n",
       "                                                                                  vocabulary=None))],\n",
       "                                                          verbose=False))],\n",
       "                              transformer_weights=None, verbose=False)),\n",
       "                ('model',\n",
       "                 SVC(C=2, break_ties=False, cache_size=200, class_weight=None,\n",
       "                     coef0=0.0, decision_function_shape='ovr', degree=3,\n",
       "                     gamma='scale', kernel='linear', max_iter=-1,\n",
       "                     probability=True, random_state=None, shrinking=True,\n",
       "                     tol=0.001, verbose=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_optimal_pipeline.fit(variables[['creator_department','resource_type','comment']], response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "upset-orientation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.93592181, 0.00792153, 0.0059089 , 0.00453867, 0.00364923,\n",
       "        0.04205986],\n",
       "       [0.00487574, 0.0643802 , 0.83370136, 0.00512442, 0.00365564,\n",
       "        0.08826265]])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_optimal_pipeline.predict_proba(proccessed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Custom User Feedback",
   "language": "python",
   "name": "user_feedback"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
